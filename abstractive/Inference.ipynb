{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4kO8Iqp1Da2-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "summary = pd.read_csv('./news_summary.csv',\n",
    "                      encoding='iso-8859-1')\n",
    "raw = pd.read_csv('./news_summary_more.csv',\n",
    "                  encoding='iso-8859-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "RNDkwi_F6ANa",
    "outputId": "f9a0b199-1f3f-494b-9337-4144b04541ac"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headlines</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>upGrad learner switches to career in ML &amp; Al w...</td>\n",
       "      <td>Saurav Kant, an alumnus of upGrad and IIIT-B's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Delhi techie wins free food from Swiggy for on...</td>\n",
       "      <td>Kunal Shah's credit card bill payment platform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>New Zealand end Rohit Sharma-led India's 12-ma...</td>\n",
       "      <td>New Zealand defeated India by 8 wickets in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aegon life iTerm insurance plan helps customer...</td>\n",
       "      <td>With Aegon Life iTerm Insurance plan, customer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Have known Hirani for yrs, what if MeToo claim...</td>\n",
       "      <td>Speaking about the sexual harassment allegatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98396</th>\n",
       "      <td>CRPF jawan axed to death by Maoists in Chhatti...</td>\n",
       "      <td>A CRPF jawan was on Tuesday axed to death with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98397</th>\n",
       "      <td>First song from Sonakshi Sinha's 'Noor' titled...</td>\n",
       "      <td>'Uff Yeh', the first song from the Sonakshi Si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98398</th>\n",
       "      <td>'The Matrix' film to get a reboot: Reports</td>\n",
       "      <td>According to reports, a new version of the 199...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98399</th>\n",
       "      <td>Snoop Dogg aims gun at clown dressed as Trump ...</td>\n",
       "      <td>A new music video shows rapper Snoop Dogg aimi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98400</th>\n",
       "      <td>Madhesi Morcha withdraws support to Nepalese g...</td>\n",
       "      <td>Madhesi Morcha, an alliance of seven political...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98401 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               headlines  \\\n",
       "0      upGrad learner switches to career in ML & Al w...   \n",
       "1      Delhi techie wins free food from Swiggy for on...   \n",
       "2      New Zealand end Rohit Sharma-led India's 12-ma...   \n",
       "3      Aegon life iTerm insurance plan helps customer...   \n",
       "4      Have known Hirani for yrs, what if MeToo claim...   \n",
       "...                                                  ...   \n",
       "98396  CRPF jawan axed to death by Maoists in Chhatti...   \n",
       "98397  First song from Sonakshi Sinha's 'Noor' titled...   \n",
       "98398         'The Matrix' film to get a reboot: Reports   \n",
       "98399  Snoop Dogg aims gun at clown dressed as Trump ...   \n",
       "98400  Madhesi Morcha withdraws support to Nepalese g...   \n",
       "\n",
       "                                                    text  \n",
       "0      Saurav Kant, an alumnus of upGrad and IIIT-B's...  \n",
       "1      Kunal Shah's credit card bill payment platform...  \n",
       "2      New Zealand defeated India by 8 wickets in the...  \n",
       "3      With Aegon Life iTerm Insurance plan, customer...  \n",
       "4      Speaking about the sexual harassment allegatio...  \n",
       "...                                                  ...  \n",
       "98396  A CRPF jawan was on Tuesday axed to death with...  \n",
       "98397  'Uff Yeh', the first song from the Sonakshi Si...  \n",
       "98398  According to reports, a new version of the 199...  \n",
       "98399  A new music video shows rapper Snoop Dogg aimi...  \n",
       "98400  Madhesi Morcha, an alliance of seven political...  \n",
       "\n",
       "[98401 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "P6HfGquWEBSF"
   },
   "outputs": [],
   "source": [
    "pre1 = raw.iloc[:, 0:2].copy()\n",
    "pre2 = summary.iloc[:, 0:6].copy()\n",
    "\n",
    "# To increase the intake of possible text values to build a reliable model\n",
    "pre2['text'] = pre2['author'].str.cat(pre2['date'\n",
    "        ].str.cat(pre2['read_more'].str.cat(pre2['text'\n",
    "        ].str.cat(pre2['ctext'], sep=' '), sep=' '), sep=' '), sep=' ')\n",
    "\n",
    "pre = pd.DataFrame()\n",
    "pre['text'] = pd.concat([pre1['text'], pre2['text']], ignore_index=True)\n",
    "pre['summary'] = pd.concat([pre1['headlines'], pre2['headlines']],\n",
    "                           ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "Pup4ZOL0EDSr",
    "outputId": "148d10f5-bb0b-4d8f-b7d9-4af7a41f3814"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Saurav Kant, an alumnus of upGrad and IIIT-B's...</td>\n",
       "      <td>upGrad learner switches to career in ML &amp; Al w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kunal Shah's credit card bill payment platform...</td>\n",
       "      <td>Delhi techie wins free food from Swiggy for on...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Saurav Kant, an alumnus of upGrad and IIIT-B's...   \n",
       "1  Kunal Shah's credit card bill payment platform...   \n",
       "\n",
       "                                             summary  \n",
       "0  upGrad learner switches to career in ML & Al w...  \n",
       "1  Delhi techie wins free food from Swiggy for on...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Mnumd1kxELGi"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Remove non-alphabetic characters (Data Cleaning)\n",
    "def text_strip(column):\n",
    "    # print(column)\n",
    "    # print('============')\n",
    "    for row in column:\n",
    "        # print(0, row)\n",
    "        row = re.sub(\"(\\\\t)\", \" \", str(row)).lower()\n",
    "        # print(1, row)\n",
    "        row = re.sub(\"(\\\\r)\", \" \", str(row)).lower()\n",
    "        # print(2, row)\n",
    "        row = re.sub(\"(\\\\n)\", \" \", str(row)).lower()\n",
    "        # print(3, row)\n",
    "        # Remove _ if it occurs more than one time consecutively\n",
    "        row = re.sub(\"(__+)\", \" \", str(row)).lower()\n",
    "        # print(4, row)\n",
    "        # Remove - if it occurs more than one time consecutively\n",
    "        row = re.sub(\"(--+)\", \" \", str(row)).lower()\n",
    "        # print(5, row)\n",
    "        # Remove ~ if it occurs more than one time consecutively\n",
    "        row = re.sub(\"(~~+)\", \" \", str(row)).lower()\n",
    "        # print(6, row)\n",
    "        # Remove + if it occurs more than one time consecutively\n",
    "        row = re.sub(\"(\\+\\++)\", \" \", str(row)).lower()\n",
    "        # print(7, row)\n",
    "        # Remove . if it occurs more than one time consecutively\n",
    "        row = re.sub(\"(\\.\\.+)\", \" \", str(row)).lower()\n",
    "        # print(8,row)\n",
    "        # Remove the characters - <>()|&©ø\"',;?~*!\n",
    "        row = re.sub(r\"[<>()|&©ø\\[\\]\\'\\\",;?~*!]\", \" \", str(row)).lower()\n",
    "        # print(9, row)\n",
    "        # Remove mailto:\n",
    "        row = re.sub(\"(mailto:)\", \" \", str(row)).lower()\n",
    "        # print(10, row)\n",
    "        # Remove \\x9* in text\n",
    "        row = re.sub(r\"(\\\\x9\\d)\", \" \", str(row)).lower()\n",
    "        # print(11, row)\n",
    "        # Replace INC nums to INC_NUM\n",
    "        row = re.sub(\"([iI][nN][cC]\\d+)\", \"INC_NUM\", str(row)).lower()\n",
    "        # print(12, row)\n",
    "        # Replace CM# and CHG# to CM_NUM\n",
    "        row = re.sub(\"([cC][mM]\\d+)|([cC][hH][gG]\\d+)\", \"CM_NUM\", str(row)).lower()\n",
    "        # print(13, row)\n",
    "        # Remove punctuations at the end of a word\n",
    "        row = re.sub(\"(\\.\\s+)\", \" \", str(row)).lower()\n",
    "        row = re.sub(\"(\\-\\s+)\", \" \", str(row)).lower()\n",
    "        row = re.sub(\"(\\:\\s+)\", \" \", str(row)).lower()\n",
    "        # print(14, row)\n",
    "        # Replace any url to only the domain name\n",
    "        try:\n",
    "            url = re.search(r\"((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)\", str(row))\n",
    "            repl_url = url.group(3)\n",
    "            row = re.sub(r\"((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)\", repl_url, str(row))\n",
    "            # print(15, row)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Remove multiple spaces\n",
    "        row = re.sub(\"(\\s+)\", \" \", str(row)).lower()\n",
    "        # print(16, row)\n",
    "        # Remove the single character hanging between any two spaces\n",
    "        row = re.sub(\"(\\s+.\\s+)\", \" \", str(row)).lower()\n",
    "        # print(17, row)\n",
    "\n",
    "        yield row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c-oZabvnL9gf",
    "outputId": "5dcd6819-173c-4696-dbad-a435790b84bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saurav Kant, an alumnus of upGrad and IIIT-B's PG Program in Machine learning and Artificial Intelligence, was a Sr Systems Engineer at Infosys with almost 5 years of work experience. The program and upGrad's 360-degree career support helped him transition to a Data Scientist at Tech Mahindra with 90% salary hike. upGrad's Online Power Learning has powered 3 lakh+ careers.\n"
     ]
    }
   ],
   "source": [
    "print(pre['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZrNr5iomEQxM",
    "outputId": "7ff174f3-67d0-429b-a432-4abec9c14cde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102915\n",
      "102915\n"
     ]
    }
   ],
   "source": [
    "processed_text = text_strip(pre['text'][:100])\n",
    "print(len(pre['text']))\n",
    "processed_summary = text_strip(pre['summary'][:100])\n",
    "print(len(pre['summary']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q6tjxIlReYJh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJ5uyXoCJ-2W",
    "outputId": "87f1a98d-5c17-4ade-d9b1-f737b339eb89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object text_strip at 0x000001E22F72F120>\n"
     ]
    }
   ],
   "source": [
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "69VlCMPJGttA",
    "outputId": "933d4354-e183-4c69-a795-c859ceb3ea04"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from time import time\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser']) \n",
    "\n",
    "# Process text as batches and yield Doc objects in order\n",
    "text = [str(doc) for doc in nlp.pipe(processed_text, batch_size=5000)]\n",
    "\n",
    "summary = ['_START_ '+ str(doc) + ' _END_' for doc in nlp.pipe(processed_summary, batch_size=5000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "rSgwRf7RLxVT",
    "outputId": "4ac769f3-3421-45ea-8b1e-887c1a3630dc"
   },
   "outputs": [],
   "source": [
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "gtg_ajgvoUhz",
    "outputId": "830b57af-4e59-43c1-d2e4-e848b168bf56"
   },
   "outputs": [],
   "source": [
    "summary[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QCDFuRHxoxK3"
   },
   "outputs": [],
   "source": [
    "pre['cleaned_text'] = pd.Series(text)\n",
    "pre['cleaned_summary'] = pd.Series(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "sN9iy9tOpeBC",
    "outputId": "23769aa1-2d03-4e3a-dbdd-5536037c8570"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text_count = []\n",
    "summary_count = []\n",
    "\n",
    "for sent1,sent2 in zip(pre['cleaned_text'],pre['cleaned_summary']):\n",
    "  if isinstance(sent1, str) and isinstance(sent2, str):\n",
    "    text_count.append(len(sent1.split()))\n",
    "    summary_count.append(len(sent2.split()))\n",
    "\n",
    "graph_df = pd.DataFrame() \n",
    "\n",
    "graph_df['text'] = text_count\n",
    "graph_df['summary'] = summary_count\n",
    "\n",
    "graph_df.hist(bins = 5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kd5bPaBXucQs",
    "outputId": "60238676-d436-4365-c0cd-cf6b9799abe0"
   },
   "outputs": [],
   "source": [
    "# Check how much % of text have 0-100 words\n",
    "cnt = 0\n",
    "for i in pre['cleaned_text']:\n",
    "    if isinstance(i, str) and len(i.split()) <= 100:\n",
    "        cnt = cnt + 1\n",
    "print(cnt / len(pre['cleaned_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "At9OjjyGuqsC"
   },
   "outputs": [],
   "source": [
    "# Model to summarize the text between 0-15 words for Summary and 0-100 words for Text\n",
    "max_text_len = 100\n",
    "max_summary_len = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "wqC-cv3Nuw6v",
    "outputId": "6f3b8de6-69c8-4ca1-d347-51deed67d483"
   },
   "outputs": [],
   "source": [
    "# Select the Summaries and Text which fall below max length \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "cleaned_text = np.array(pre['cleaned_text'])\n",
    "cleaned_summary= np.array(pre['cleaned_summary'])\n",
    "\n",
    "short_text = []\n",
    "short_summary = []\n",
    "\n",
    "for i in range(len(cleaned_text)):\n",
    "    if isinstance(cleaned_text[i], str) and isinstance(cleaned_summary[i], str) and len(cleaned_summary[i].split()) <= max_summary_len and len(cleaned_text[i].split()) <= max_text_len:\n",
    "        short_text.append(cleaned_text[i])\n",
    "        short_summary.append(cleaned_summary[i])\n",
    "        \n",
    "post_pre = pd.DataFrame({'text': short_text,'summary': short_summary})\n",
    "\n",
    "post_pre.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "ZOr2DuZJvoT8",
    "outputId": "3c0a751b-a7b4-4d01-c006-273f6643b46c"
   },
   "outputs": [],
   "source": [
    "# Add sostok and eostok\n",
    "\n",
    "post_pre['summary'] = post_pre['summary'].apply(lambda x: 'sostok ' + x \\\n",
    "        + ' eostok')\n",
    "\n",
    "post_pre.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rY4j4RpevwfP"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_tr, x_val, y_tr, y_val = train_test_split(\n",
    "    np.array(post_pre[\"text\"]),\n",
    "    np.array(post_pre[\"summary\"] ),\n",
    "    test_size=0.1,\n",
    "    random_state=0,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L0qOQwjSv6v0"
   },
   "outputs": [],
   "source": [
    "# Tokenize the text to get the vocab count \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Prepare a tokenizer on training data\n",
    "x_tokenizer = Tokenizer() \n",
    "x_tokenizer.fit_on_texts(list(x_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZgkTo7_QwDt0",
    "outputId": "086c19f6-08c9-41ab-9133-c7df184239a1"
   },
   "outputs": [],
   "source": [
    "thresh = 3\n",
    "\n",
    "cnt = 0\n",
    "tot_cnt = 0\n",
    "\n",
    "for key, value in x_tokenizer.word_counts.items():\n",
    "    tot_cnt = tot_cnt + 1\n",
    "    if value < thresh:\n",
    "        cnt = cnt + 1\n",
    "    \n",
    "print(\"% of rare words in vocabulary: \", (cnt / tot_cnt) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "usM59epswXyS",
    "outputId": "d54908bd-a674-450a-a025-5175830379cc"
   },
   "outputs": [],
   "source": [
    "# Prepare a tokenizer, again -- by not considering the rare words\n",
    "x_tokenizer = Tokenizer(num_words = tot_cnt - cnt) \n",
    "x_tokenizer.fit_on_texts(list(x_tr))\n",
    "\n",
    "# Convert text sequences to integer sequences \n",
    "x_tr_seq = x_tokenizer.texts_to_sequences(x_tr) \n",
    "x_val_seq = x_tokenizer.texts_to_sequences(x_val)\n",
    "\n",
    "# Pad zero upto maximum length\n",
    "x_tr = pad_sequences(x_tr_seq,  maxlen=max_text_len, padding='post')\n",
    "x_val = pad_sequences(x_val_seq, maxlen=max_text_len, padding='post')\n",
    "\n",
    "# Size of vocabulary (+1 for padding token)\n",
    "x_voc = x_tokenizer.num_words + 1\n",
    "\n",
    "print(\"Size of vocabulary in X = {}\".format(x_voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mZsL6tfGwgVx",
    "outputId": "630bcdce-db44-4a98-848a-9c3b1a74afb6"
   },
   "outputs": [],
   "source": [
    "# Prepare a tokenizer on testing data\n",
    "y_tokenizer = Tokenizer()   \n",
    "y_tokenizer.fit_on_texts(list(y_tr))\n",
    "\n",
    "thresh = 3\n",
    "\n",
    "cnt = 0\n",
    "tot_cnt = 0\n",
    "\n",
    "for key, value in y_tokenizer.word_counts.items():\n",
    "    tot_cnt = tot_cnt + 1\n",
    "    if value < thresh:\n",
    "        cnt = cnt + 1\n",
    "    \n",
    "print(\"% of rare words in vocabulary:\",(cnt / tot_cnt) * 100)\n",
    "\n",
    "# Prepare a tokenizer, again -- by not considering the rare words\n",
    "y_tokenizer = Tokenizer(num_words=tot_cnt-cnt) \n",
    "y_tokenizer.fit_on_texts(list(y_tr))\n",
    "\n",
    "# Convert text sequences to integer sequences \n",
    "y_tr_seq = y_tokenizer.texts_to_sequences(y_tr) \n",
    "y_val_seq = y_tokenizer.texts_to_sequences(y_val) \n",
    "\n",
    "# Pad zero upto maximum length\n",
    "y_tr = pad_sequences(y_tr_seq, maxlen=max_summary_len, padding='post')\n",
    "y_val = pad_sequences(y_val_seq, maxlen=max_summary_len, padding='post')\n",
    "\n",
    "# Size of vocabulary (+1 for padding token)\n",
    "y_voc = y_tokenizer.num_words + 1\n",
    "\n",
    "print(\"Size of vocabulary in Y = {}\".format(y_voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h3wgm0EDwyQ5"
   },
   "outputs": [],
   "source": [
    "# Remove empty Summaries, .i.e, which only have 'START' and 'END' tokens\n",
    "ind = []\n",
    "\n",
    "for i in range(len(y_tr)):\n",
    "    cnt = 0\n",
    "    for j in y_tr[i]:\n",
    "        if j != 0:\n",
    "            cnt = cnt + 1\n",
    "    if cnt == 2:\n",
    "        ind.append(i)\n",
    "\n",
    "y_tr = np.delete(y_tr, ind, axis=0)\n",
    "x_tr = np.delete(x_tr, ind, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bHxatsMjw34z"
   },
   "outputs": [],
   "source": [
    "# Remove empty Summaries, .i.e, which only have 'START' and 'END' tokens\n",
    "ind = []\n",
    "for i in range(len(y_val)):\n",
    "    cnt = 0\n",
    "    for j in y_val[i]:\n",
    "        if j != 0:\n",
    "            cnt = cnt + 1\n",
    "    if cnt == 2:\n",
    "        ind.append(i)\n",
    "\n",
    "y_val = np.delete(y_val, ind, axis=0)\n",
    "x_val = np.delete(x_val, ind, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "svtBJqUdxSRP"
   },
   "source": [
    "# TRAINING PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HNX_uR2bxVyG"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, \\\n",
    "    Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P00_9nF90paD"
   },
   "source": [
    "### Inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pr_Tsy9t0vc2",
    "outputId": "233b1f68-eb3d-461b-b1ff-ae229487c4f8"
   },
   "outputs": [],
   "source": [
    "print(x_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SBvC30M3nIX8",
    "outputId": "7ba3d379-9fe1-495c-eeff-77fbc508657b"
   },
   "outputs": [],
   "source": [
    "x_tr[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zKjea_mByoGV"
   },
   "outputs": [],
   "source": [
    "# Dictionary Loading\n",
    "import pickle \n",
    "\n",
    "with open('tword.pickle', 'rb') as handle:\n",
    "    target_word_index = pickle.load(handle)\n",
    "\n",
    "with open('revtword.pickle', 'rb') as handle:\n",
    "    reverse_target_word_index = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AyeLolhZprKL",
    "outputId": "29d71d90-6aac-430f-ed8c-24bd28390a23"
   },
   "outputs": [],
   "source": [
    "# ML Models\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "decoder_model=load_model('decoder_pickle.h5')\n",
    "encoder_model=load_model('encoder_pickle.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ju0Q5bbU2Crv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nfVjpBPOq7Qr"
   },
   "outputs": [],
   "source": [
    "def dek_sek(input_seq):\n",
    "\n",
    "    # Encode the input as state vectors.\n",
    "    (e_out, e_h, e_c) = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1\n",
    "    target_seq = np.zeros((1, 1))\n",
    "\n",
    "    # Populate the first word of target sequence with the start word.\n",
    "    target_seq[0, 0] = target_word_index['sostok']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    while not stop_condition:\n",
    "        (output_tokens, h, c) = decoder_model.predict([target_seq]\n",
    "                + [e_out, e_h, e_c])\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "\n",
    "        if sampled_token != 'eostok':\n",
    "            decoded_sentence += ' ' + sampled_token\n",
    "\n",
    "        # Exit condition: either hit max length or find the stop word.\n",
    "        if sampled_token == 'eostok' or len(decoded_sentence.split()) \\\n",
    "            >= max_summary_len - 1:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1)\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update internal states\n",
    "        (e_h, e_c) = (h, c)\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lt2Xd_82q2nV",
    "outputId": "c1e560a6-8a67-4660-d790-fa5eab4e4764"
   },
   "outputs": [],
   "source": [
    "# print ('Review:', seq2text(x_tr[i]))\n",
    "# print ('Original summary:', seq2summary(y_tr[i]))\n",
    "print ('Predicted summary:', dek_sek(x_tr[i].reshape(1,\n",
    "        max_text_len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bUjhgmMr5Ugw"
   },
   "outputs": [],
   "source": [
    "def summarizer(inputText):\n",
    "\n",
    "  \n",
    "\n",
    "  summary= dek_sek(x_tr[i].reshape(1,\n",
    "        max_text_len))\n",
    "  \n",
    "  return summary"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
